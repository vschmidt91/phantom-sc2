{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combat Dataset vs Phantom Simulator\n",
    "\n",
    "Loads `resources/datasets/combat.pkl.xz`, runs each sample through `phantom/micro/simulator.py`\n",
    "(via `NumpyLanchesterSimulator`), and compares prediction quality against simple baselines."
   ],
   "id": "157cf649829223d1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T23:49:14.424249400Z",
     "start_time": "2026-02-27T23:49:13.637599Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import lzma\n",
    "import pickle\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if not (ROOT / \"phantom\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from phantom.micro.simulator import ModelCombatSetup, NumpyLanchesterSimulator, SimulationUnit\n",
    "\n",
    "DATASET_PATH = ROOT / \"resources/datasets/combat.pkl.xz\"\n",
    "MAX_SAMPLES: int | None = None  # set e.g. 2000 for a faster pass\n",
    "SEED = 7\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "print(f\"Dataset: {DATASET_PATH}\")"
   ],
   "id": "494c6bac111e8a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: C:\\Users\\volke\\PycharmProjects\\phantom-sc2\\resources\\datasets\\combat.pkl.xz\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T23:49:14.860805600Z",
     "start_time": "2026-02-27T23:49:14.425250Z"
    }
   },
   "source": [
    "with lzma.open(DATASET_PATH, \"rb\") as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "indices = np.arange(len(raw_data))\n",
    "if MAX_SAMPLES is not None and MAX_SAMPLES < len(indices):\n",
    "    indices = rng.choice(indices, size=MAX_SAMPLES, replace=False)\n",
    "\n",
    "data = [raw_data[i] for i in indices]\n",
    "print(f\"Loaded {len(data):,} samples (from {len(raw_data):,} total)\")\n",
    "print(\"Example keys:\", sorted(data[0].keys()))\n",
    "print(\"Serialized unit keys:\", sorted(data[0][\"units\"][0].keys()))"
   ],
   "id": "1ed90a2a40bda856",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\volke\\\\PycharmProjects\\\\phantom-sc2\\\\resources\\\\datasets\\\\combat.pkl.xz'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mlzma\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATASET_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      2\u001B[39m     raw_data = pickle.load(f)\n\u001B[32m      4\u001B[39m indices = np.arange(\u001B[38;5;28mlen\u001B[39m(raw_data))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\lzma.py:309\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(filename, mode, format, check, preset, filters, encoding, errors, newline)\u001B[39m\n\u001B[32m    306\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mArgument \u001B[39m\u001B[33m'\u001B[39m\u001B[33mnewline\u001B[39m\u001B[33m'\u001B[39m\u001B[33m not supported in binary mode\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    308\u001B[39m lz_mode = mode.replace(\u001B[33m\"\u001B[39m\u001B[33mt\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m309\u001B[39m binary_file = \u001B[43mLZMAFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlz_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheck\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m                       \u001B[49m\u001B[43mpreset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpreset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m    313\u001B[39m     encoding = io.text_encoding(encoding)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\lzma.py:120\u001B[39m, in \u001B[36mLZMAFile.__init__\u001B[39m\u001B[34m(self, filename, mode, format, check, preset, filters)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m    119\u001B[39m     mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[38;5;28mself\u001B[39m._fp = \u001B[43mbuiltins\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28mself\u001B[39m._closefp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    122\u001B[39m \u001B[38;5;28mself\u001B[39m._mode = mode_code\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\volke\\\\PycharmProjects\\\\phantom-sc2\\\\resources\\\\datasets\\\\combat.pkl.xz'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class NotebookParameters:\n",
    "    time_distribution_lambda: float = 1.0\n",
    "    lancester_dimension: float = 1.5\n",
    "    enemy_range_bonus: float = 1.0\n",
    "\n",
    "\n",
    "def to_simulation_unit(u: dict) -> SimulationUnit:\n",
    "    return SimulationUnit(\n",
    "        tag=int(u[\"tag\"]),\n",
    "        is_enemy=bool(u[\"is_enemy\"]),\n",
    "        is_flying=bool(u[\"is_flying\"]),\n",
    "        health=float(u[\"health\"]),\n",
    "        shield=float(u[\"shield\"]),\n",
    "        ground_dps=float(u[\"ground_dps\"]),\n",
    "        air_dps=float(u[\"air_dps\"]),\n",
    "        ground_range=float(u[\"ground_range\"]),\n",
    "        air_range=float(u[\"air_range\"]),\n",
    "        radius=float(u[\"radius\"]),\n",
    "        real_speed=float(u[\"real_speed\"]),\n",
    "        position=(float(u[\"position\"][0]), float(u[\"position\"][1])),\n",
    "    )\n",
    "\n",
    "\n",
    "def to_model_setup(sample: dict) -> ModelCombatSetup:\n",
    "    units1: list[SimulationUnit] = []\n",
    "    units2: list[SimulationUnit] = []\n",
    "    for unit_dict in sample[\"units\"]:\n",
    "        unit = to_simulation_unit(unit_dict)\n",
    "        if unit.is_enemy:\n",
    "            units2.append(unit)\n",
    "        else:\n",
    "            units1.append(unit)\n",
    "    attacking = set[int]()\n",
    "    attacking.update(u.tag for u in units1)\n",
    "    attacking.update(u.tag for u in units2)\n",
    "    return ModelCombatSetup(\n",
    "        units1=units1,\n",
    "        units2=units2,\n",
    "        attacking=attacking,\n",
    "    )\n",
    "\n",
    "\n",
    "def sum_hp(units: list[SimulationUnit]) -> float:\n",
    "    return float(sum(u.health + u.shield for u in units))\n",
    "\n",
    "\n",
    "def sum_dps(units: list[SimulationUnit]) -> float:\n",
    "    return float(sum(max(u.ground_dps, u.air_dps) for u in units))\n",
    "\n",
    "\n",
    "def sum_force(units: list[SimulationUnit], lanchester_power: float) -> float:\n",
    "    return float(\n",
    "        sum(\n",
    "            ((u.health + u.shield) ** lanchester_power) * max(u.ground_dps, u.air_dps)\n",
    "            for u in units\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "sim = NumpyLanchesterSimulator(NotebookParameters(), num_steps=10)"
   ],
   "id": "104608d0b31531da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rows = []\n",
    "for sample in data:\n",
    "    setup = to_model_setup(sample)\n",
    "    if not setup.units1 or not setup.units2:\n",
    "        continue\n",
    "\n",
    "    pred = sim.simulate(setup).outcome_global\n",
    "    hp1 = sum_hp(setup.units1)\n",
    "    hp2 = sum_hp(setup.units2)\n",
    "    dps1 = sum_dps(setup.units1)\n",
    "    dps2 = sum_dps(setup.units2)\n",
    "    force1 = sum_force(setup.units1, sim.parameters.lancester_dimension)\n",
    "    force2 = sum_force(setup.units2, sim.parameters.lancester_dimension)\n",
    "\n",
    "    rows.append({\n",
    "        \"true_raw\": float(sample[\"advantage_log\"]),\n",
    "        \"pred_sim\": float(pred),\n",
    "        \"hp_ratio\": (hp1 - hp2) / max(1e-9, hp1 + hp2),\n",
    "        \"dps_ratio\": (dps1 - dps2) / max(1e-9, dps1 + dps2),\n",
    "        \"force\": (force1 - force2) / max(1e-9, force1 + force2),\n",
    "        \"n1\": len(setup.units1),\n",
    "        \"n2\": len(setup.units2),\n",
    "        \"hp1\": hp1,\n",
    "        \"hp2\": hp2,\n",
    "    })\n",
    "\n",
    "print(f\"Evaluated {len(rows):,} valid samples\")\n",
    "rows[:2]"
   ],
   "id": "61007de2b191c736",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "true_raw = np.array([r[\"true_raw\"] for r in rows], dtype=float)\n",
    "pred_sim = np.array([r[\"pred_sim\"] for r in rows], dtype=float)\n",
    "hp_ratio = np.array([r[\"hp_ratio\"] for r in rows], dtype=float)\n",
    "dps_ratio = np.array([r[\"dps_ratio\"] for r in rows], dtype=float)\n",
    "force = np.array([r[\"force\"] for r in rows], dtype=float)\n",
    "\n",
    "def sign_no_zero(x: np.ndarray) -> np.ndarray:\n",
    "    y = np.sign(x)\n",
    "    y[y == 0] = 1\n",
    "    return y\n",
    "\n",
    "y_sign = sign_no_zero(true_raw)\n",
    "\n",
    "def metrics_for(name: str, pred: np.ndarray) -> dict:\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"sign_accuracy\": float((sign_no_zero(pred) == y_sign).mean()),\n",
    "        \"corr_to_true_raw\": float(np.corrcoef(pred, true_raw)[0, 1]),\n",
    "        \"mae_to_true_raw\": float(np.mean(np.abs(pred - true_raw))),\n",
    "    }\n",
    "\n",
    "metrics = [\n",
    "    metrics_for(\"simulator\", pred_sim),\n",
    "    metrics_for(\"hp_ratio\", hp_ratio),\n",
    "    metrics_for(\"dps_ratio\", dps_ratio),\n",
    "    metrics_for(\"force\", force),\n",
    "]\n",
    "metrics"
   ],
   "id": "2feee73eb692ea79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pred_range = (-2, 2)\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\n",
    "        \"Raw Outcome Distribution\",\n",
    "        \"Predicted vs True (raw outcomes)\",\n",
    "        \"Sign Accuracy Comparison\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=true_raw, nbinsx=40, name=\"true_raw\", opacity=0.8),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=pred_sim, nbinsx=40, name=\"simulator\", opacity=0.6),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=true_raw,\n",
    "        y=pred_sim,\n",
    "        mode=\"markers\",\n",
    "        name=\"simulator\",\n",
    "        marker={\"size\": 5, \"opacity\": 0.35},\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=true_raw,\n",
    "        y=hp_ratio,\n",
    "        mode=\"markers\",\n",
    "        name=\"hp_ratio\",\n",
    "        marker={\"size\": 5, \"opacity\": 0.25},\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=true_raw,\n",
    "        y=dps_ratio,\n",
    "        mode=\"markers\",\n",
    "        name=\"dps_ratio\",\n",
    "        marker={\"size\": 5, \"opacity\": 0.25},\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=true_raw,\n",
    "        y=force,\n",
    "        mode=\"markers\",\n",
    "        name=\"force\",\n",
    "        marker={\"size\": 5, \"opacity\": 0.25},\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "models = [m[\"model\"] for m in metrics]\n",
    "acc = [m[\"sign_accuracy\"] for m in metrics]\n",
    "fig.add_trace(\n",
    "    go.Bar(x=models, y=acc, name=\"accuracy\", text=[f\"{v:.3f}\" for v in acc], textposition=\"outside\"),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"value\", range=list(pred_range), row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"true_raw\", range=list(pred_range), row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"model\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"prediction\", range=list(pred_range), row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"accuracy\", range=[0.0, 1.0], row=1, col=3)\n",
    "fig.update_layout(height=450, width=1400, barmode=\"overlay\", template=\"plotly_white\")\n",
    "fig.show()"
   ],
   "id": "a5358f08c8be7bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bins = np.linspace(pred_sim.min(), pred_sim.max(), 11)\n",
    "digitized = np.digitize(pred_sim, bins)\n",
    "calibration = []\n",
    "for b in range(1, len(bins)):\n",
    "    mask = digitized == b\n",
    "    count = int(mask.sum())\n",
    "    if count == 0:\n",
    "        continue\n",
    "    calibration.append({\n",
    "        \"bin_center\": float((bins[b - 1] + bins[b]) / 2),\n",
    "        \"mean_pred\": float(pred_sim[mask].mean()),\n",
    "        \"mean_true\": float(true_raw[mask].mean()),\n",
    "        \"count\": count,\n",
    "    })\n",
    "\n",
    "cal_pred = np.array([c[\"mean_pred\"] for c in calibration])\n",
    "cal_true = np.array([c[\"mean_true\"] for c in calibration])\n",
    "cal_count = np.array([c[\"count\"] for c in calibration])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cal_pred,\n",
    "        y=cal_true,\n",
    "        mode=\"lines+markers+text\",\n",
    "        text=[str(int(c)) for c in cal_count],\n",
    "        textposition=\"top center\",\n",
    "        name=\"simulator\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Simulator Calibration on Raw Outcomes\",\n",
    "    xaxis_title=\"mean predicted\",\n",
    "    yaxis_title=\"mean true_raw\",\n",
    "    width=600,\n",
    "    height=500,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "calibration"
   ],
   "id": "38235793bef1dbfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Comparisons are on raw outcome values (`result`) without normalization.\n",
    "- `sign_accuracy` compares winner direction only (`result > 0` means `units1` wins).\n",
    "- Absolute-scale metrics (`mae_to_true_raw`) are included as-is; known scale mismatch is a TODO.\n",
    "- You can tune `NotebookParameters` to test alternative simulator parameter sets."
   ],
   "id": "86f1bbfe04bbcb51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
